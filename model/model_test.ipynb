{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "from einops import rearrange\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.nn.utils.weight_norm import WeightNorm\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-FNO 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, factor, ff_weight_norm, n_layers, layer_norm, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(n_layers):\n",
    "            in_dim = dim if i == 0 else dim * factor\n",
    "            out_dim = dim if i == n_layers - 1 else dim * factor\n",
    "            self.layers.append(nn.Sequential(\n",
    "                WNLinear(in_dim, out_dim, wnorm=ff_weight_norm),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(inplace=True) if i < n_layers - 1 else nn.Identity(),\n",
    "                nn.LayerNorm(out_dim) if layer_norm and i == n_layers -\n",
    "                1 else nn.Identity(),\n",
    "            ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class WNLinear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None, wnorm=False):\n",
    "        super().__init__(in_features=in_features,\n",
    "                         out_features=out_features,\n",
    "                         bias=bias,\n",
    "                         device=device,\n",
    "                         dtype=dtype)\n",
    "        if wnorm:\n",
    "            weight_norm(self)\n",
    "\n",
    "        self._fix_weight_norm_deepcopy()\n",
    "\n",
    "    def _fix_weight_norm_deepcopy(self):\n",
    "        # Fix bug where deepcopy doesn't work with weightnorm.\n",
    "        # Taken from https://github.com/pytorch/pytorch/issues/28594#issuecomment-679534348\n",
    "        orig_deepcopy = getattr(self, '__deepcopy__', None)\n",
    "\n",
    "        def __deepcopy__(self, memo):\n",
    "            # save and delete all weightnorm weights on self\n",
    "            weights = {}\n",
    "            for hook in self._forward_pre_hooks.values():\n",
    "                if isinstance(hook, WeightNorm):\n",
    "                    weights[hook.name] = getattr(self, hook.name)\n",
    "                    delattr(self, hook.name)\n",
    "            # remove this deepcopy method, restoring the object's original one if necessary\n",
    "            __deepcopy__ = self.__deepcopy__\n",
    "            if orig_deepcopy:\n",
    "                self.__deepcopy__ = orig_deepcopy\n",
    "            else:\n",
    "                del self.__deepcopy__\n",
    "            # actually do the copy\n",
    "            result = copy.deepcopy(self)\n",
    "            # restore weights and method on self\n",
    "            for name, value in weights.items():\n",
    "                setattr(self, name, value)\n",
    "            self.__deepcopy__ = __deepcopy__\n",
    "            return result\n",
    "        # bind __deepcopy__ to the weightnorm'd layer\n",
    "        self.__deepcopy__ = __deepcopy__.__get__(self, self.__class__)\n",
    "\n",
    "\n",
    "\n",
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, n_modes, forecast_ff, backcast_ff,\n",
    "                 fourier_weight, factor, ff_weight_norm,\n",
    "                 n_ff_layers, layer_norm, use_fork, dropout, mode):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_modes = n_modes\n",
    "        self.mode = mode\n",
    "        self.use_fork = use_fork\n",
    "\n",
    "        self.fourier_weight = fourier_weight\n",
    "        # Can't use complex type yet. See https://github.com/pytorch/pytorch/issues/59998\n",
    "        if not self.fourier_weight:\n",
    "            self.fourier_weight = nn.ParameterList([])\n",
    "            for _ in range(2):\n",
    "                weight = torch.FloatTensor(in_dim, out_dim, n_modes, 2)\n",
    "                param = nn.Parameter(weight)\n",
    "                nn.init.xavier_normal_(param)\n",
    "                self.fourier_weight.append(param)\n",
    "\n",
    "        if use_fork:\n",
    "            self.forecast_ff = forecast_ff\n",
    "            if not self.forecast_ff:\n",
    "                self.forecast_ff = FeedForward(\n",
    "                    out_dim, factor, ff_weight_norm, n_ff_layers, layer_norm, dropout)\n",
    "\n",
    "        self.backcast_ff = backcast_ff\n",
    "        if not self.backcast_ff:\n",
    "            self.backcast_ff = FeedForward(\n",
    "                out_dim, factor, ff_weight_norm, n_ff_layers, layer_norm, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape == [batch_size, grid_size, grid_size, in_dim]\n",
    "        if self.mode != 'no-fourier':\n",
    "            x = self.forward_fourier(x)\n",
    "\n",
    "        b = self.backcast_ff(x)\n",
    "        f = self.forecast_ff(x) if self.use_fork else None\n",
    "        return b, f\n",
    "\n",
    "    def forward_fourier(self, x):\n",
    "        x = rearrange(x, 'b x i -> b i x')\n",
    "        # x.shape == [batch_size, in_dim, grid_size]\n",
    "\n",
    "        B, I, N = x.shape\n",
    "\n",
    "        # # # Dimesion X # # #\n",
    "        x_ft = torch.fft.rfft(x, dim=-1, norm='ortho')\n",
    "        # x_ft.shape == [batch_size, in_dim, grid_size, grid_size // 2 + 1]\n",
    "\n",
    "        out_ft = x_ft.new_zeros(B, I, N // 2 + 1)\n",
    "        # out_ft.shape == [batch_size, in_dim, grid_size, grid_size // 2 + 1, 2]\n",
    "        print(out_ft.shape)\n",
    "        print(x_ft.shape)\n",
    "        if self.mode == 'full':\n",
    "            out_ft[:, :, :self.n_modes] = torch.einsum(\n",
    "                \"bix,iox->box\",\n",
    "                x_ft[:, :, :self.n_modes],\n",
    "                torch.view_as_complex(self.fourier_weight[0]))\n",
    "        elif self.mode == 'low-pass':\n",
    "            out_ft[:, :, :self.n_modes] = x_ft[:, :, :self.n_modes]\n",
    "\n",
    "        x = torch.fft.irfft(out_ft, n=N, dim=-1, norm='ortho')\n",
    "        # x.shape == [batch_size, in_dim, grid_size, grid_size]\n",
    "\n",
    "\n",
    "\n",
    "        x = rearrange(x, 'b i x -> b x i')\n",
    "        # x.shape == [batch_size, grid_size, grid_size, out_dim]\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FNOFactorized1DBlock(nn.Module):\n",
    "    def __init__(self, modes, width, input_dim=12, dropout=0.0, in_dropout=0.0,\n",
    "                 n_layers=4, share_weight: bool = False,\n",
    "                 share_fork=False, factor=2,\n",
    "                 ff_weight_norm=False, n_ff_layers=2,\n",
    "                 gain=1, layer_norm=False, use_fork=False, mode='full'):\n",
    "        super().__init__()\n",
    "        self.modes = modes\n",
    "        self.width = width\n",
    "        self.input_dim = input_dim\n",
    "        self.in_proj = WNLinear(input_dim, self.width, wnorm=ff_weight_norm)\n",
    "        self.drop = nn.Dropout(in_dropout)\n",
    "        self.n_layers = n_layers\n",
    "        self.use_fork = use_fork\n",
    "\n",
    "        self.forecast_ff = self.backcast_ff = None\n",
    "        if share_fork:\n",
    "            if use_fork:\n",
    "                self.forecast_ff = FeedForward(\n",
    "                    width, factor, ff_weight_norm, n_ff_layers, layer_norm, dropout)\n",
    "            self.backcast_ff = FeedForward(\n",
    "                width, factor, ff_weight_norm, n_ff_layers, layer_norm, dropout)\n",
    "\n",
    "        self.fourier_weight = None\n",
    "        if share_weight:\n",
    "            self.fourier_weight = nn.ParameterList([])\n",
    "            for _ in range(2):\n",
    "                weight = torch.FloatTensor(width, width, modes, 2)\n",
    "                param = nn.Parameter(weight)\n",
    "                nn.init.xavier_normal_(param, gain=gain)\n",
    "                self.fourier_weight.append(param)\n",
    "\n",
    "        self.spectral_layers = nn.ModuleList([])\n",
    "        for _ in range(n_layers):\n",
    "            self.spectral_layers.append(SpectralConv1d(in_dim=width,\n",
    "                                                       out_dim=width,\n",
    "                                                       n_modes=modes,\n",
    "                                                       forecast_ff=self.forecast_ff,\n",
    "                                                       backcast_ff=self.backcast_ff,\n",
    "                                                       fourier_weight=self.fourier_weight,\n",
    "                                                       factor=factor,\n",
    "                                                       ff_weight_norm=ff_weight_norm,\n",
    "                                                       n_ff_layers=n_ff_layers,\n",
    "                                                       layer_norm=layer_norm,\n",
    "                                                       use_fork=use_fork,\n",
    "                                                       dropout=dropout,\n",
    "                                                       mode=mode))\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            WNLinear(self.width, 128, wnorm=ff_weight_norm),\n",
    "            WNLinear(128, 1, wnorm=ff_weight_norm))\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        # x.shape == [n_batches, *dim_sizes, input_size]\n",
    "        forecast = 0\n",
    "        x = self.in_proj(x)\n",
    "        x = self.drop(x)\n",
    "        forecast_list = []\n",
    "        for i in range(self.n_layers):\n",
    "            print(\"x-->\", x.shape)\n",
    "            layer = self.spectral_layers[i]\n",
    "            b, f = layer(x)\n",
    "\n",
    "            if self.use_fork:\n",
    "                f_out = self.out(f)\n",
    "                forecast = forecast + f_out\n",
    "                forecast_list.append(f_out)\n",
    "\n",
    "            x = x + b\n",
    "\n",
    "        if not self.use_fork:\n",
    "            forecast = self.out(b)\n",
    "\n",
    "        return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "a = False\n",
    "\n",
    "if not a:\n",
    "    print(\"YES\")\n",
    "else:\n",
    "    print(\"NO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNOFactorized1DBlock(\n",
    "    modes = 16,\n",
    "    width = 64,\n",
    "    input_dim=1,\n",
    "    dropout=0.0,\n",
    "    in_dropout=0.0,\n",
    "    n_layers=24,\n",
    "    share_weight = True,\n",
    "    \n",
    "    factor=2,\n",
    "    ff_weight_norm=True,\n",
    "    n_ff_layers=2,\n",
    "    gain=1,\n",
    "    layer_norm=False,\n",
    "    \n",
    "    share_fork=False,\n",
    "    use_fork=False,\n",
    "    mode='full'\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673346"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x--> torch.Size([16, 100, 64])\n",
      "torch.Size([16, 64, 51])\n",
      "torch.Size([16, 64, 51])\n",
      "x--> torch.Size([16, 100, 64])\n",
      "torch.Size([16, 64, 51])\n",
      "torch.Size([16, 64, 51])\n",
      "x--> torch.Size([16, 100, 64])\n",
      "torch.Size([16, 64, 51])\n",
      "torch.Size([16, 64, 51])\n",
      "x--> torch.Size([16, 100, 64])\n",
      "torch.Size([16, 64, 51])\n",
      "torch.Size([16, 64, 51])\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand(16,100,1)\n",
    "output = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"forecast\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[\"forecast_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Normalizer(nn.Module):\n",
    "    def __init__(self, size, max_accumulations=10**6, std_epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.max_accumulations = max_accumulations\n",
    "        self.register_buffer('count', torch.tensor(0.0))\n",
    "        self.register_buffer('n_accumulations', torch.tensor(0.0))\n",
    "        self.register_buffer('sum', torch.full(size, 0.0))\n",
    "        self.register_buffer('sum_squared', torch.full(size, 0.0))\n",
    "        self.register_buffer('one', torch.tensor(1.0))\n",
    "        self.register_buffer('std_epsilon', torch.full(size, std_epsilon))\n",
    "        self.dim_sizes = None\n",
    "\n",
    "    def _accumulate(self, x):\n",
    "        x_count = x.shape[0]\n",
    "        x_sum = x.sum(dim=0)\n",
    "        x_sum_squared = (x**2).sum(dim=0)\n",
    "\n",
    "        self.sum += x_sum\n",
    "        self.sum_squared += x_sum_squared\n",
    "        self.count += x_count\n",
    "        self.n_accumulations += 1\n",
    "\n",
    "    def _pool_dims(self, x):\n",
    "        _, *dim_sizes, _ = x.shape\n",
    "        self.dim_sizes = dim_sizes\n",
    "        if self.dim_sizes:\n",
    "            x = rearrange(x, 'b ... h -> (b ...) h')\n",
    "        #print(\"x_pool_dim ->\", x.shape)\n",
    "        return x\n",
    "\n",
    "    def _unpool_dims(self, x):\n",
    "        if len(self.dim_sizes) == 1:\n",
    "            x = rearrange(x, '(b m) h -> b m h', m=self.dim_sizes[0])\n",
    "            \n",
    "        elif len(self.dim_sizes) == 2:\n",
    "            m, n = self.dim_sizes\n",
    "            x = rearrange(x, '(b m n) h -> b m n h', m=m, n=n)\n",
    "        \n",
    "        #print(\"x_unpool_dim -->\", x.shape)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._pool_dims(x)\n",
    "        # x.shape == [batch_size, latent_dim]\n",
    "\n",
    "        if self.training and self.n_accumulations < self.max_accumulations:\n",
    "            self._accumulate(x)\n",
    "\n",
    "        x = (x - self.mean) / self.std\n",
    "        x = self._unpool_dims(x)\n",
    "        return x\n",
    "\n",
    "    def inverse(self, x, channel=None):\n",
    "        x = self._pool_dims(x)\n",
    "\n",
    "        if channel is None:\n",
    "            x = x * self.std + self.mean\n",
    "        else:\n",
    "            x = x * self.std[channel] + self.mean[channel]\n",
    "\n",
    "        x = self._unpool_dims(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        safe_count = max(self.count, self.one)\n",
    "        mean = self.sum / safe_count\n",
    "        #print(\"mean -->\", mean)\n",
    "        return mean\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        safe_count = max(self.count, self.one)\n",
    "        std = torch.sqrt(self.sum_squared / safe_count - self.mean**2)\n",
    "        #print(\"std -->\", std)\n",
    "        return torch.maximum(std, self.std_epsilon)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, factor, ff_weight_norm, n_layers, layer_norm, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(n_layers):\n",
    "            in_dim = dim if i == 0 else dim * factor\n",
    "            out_dim = dim if i == n_layers - 1 else dim * factor\n",
    "            self.layers.append(nn.Sequential(\n",
    "                WNLinear(in_dim, out_dim, wnorm=ff_weight_norm),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(inplace=True) if i < n_layers - 1 else nn.Identity(),\n",
    "                nn.LayerNorm(out_dim) if layer_norm and i == n_layers -\n",
    "                1 else nn.Identity(),\n",
    "            ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class WNLinear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None, wnorm=False):\n",
    "        super().__init__(in_features=in_features,\n",
    "                         out_features=out_features,\n",
    "                         bias=bias,\n",
    "                         device=device,\n",
    "                         dtype=dtype)\n",
    "        if wnorm:\n",
    "            weight_norm(self)\n",
    "\n",
    "        self._fix_weight_norm_deepcopy()\n",
    "\n",
    "    def _fix_weight_norm_deepcopy(self):\n",
    "        # Fix bug where deepcopy doesn't work with weightnorm.\n",
    "        # Taken from https://github.com/pytorch/pytorch/issues/28594#issuecomment-679534348\n",
    "        orig_deepcopy = getattr(self, '__deepcopy__', None)\n",
    "\n",
    "        def __deepcopy__(self, memo):\n",
    "            # save and delete all weightnorm weights on self\n",
    "            weights = {}\n",
    "            for hook in self._forward_pre_hooks.values():\n",
    "                if isinstance(hook, WeightNorm):\n",
    "                    weights[hook.name] = getattr(self, hook.name)\n",
    "                    delattr(self, hook.name)\n",
    "            # remove this deepcopy method, restoring the object's original one if necessary\n",
    "            __deepcopy__ = self.__deepcopy__\n",
    "            if orig_deepcopy:\n",
    "                self.__deepcopy__ = orig_deepcopy\n",
    "            else:\n",
    "                del self.__deepcopy__\n",
    "            # actually do the copy\n",
    "            result = copy.deepcopy(self)\n",
    "            # restore weights and method on self\n",
    "            for name, value in weights.items():\n",
    "                setattr(self, name, value)\n",
    "            self.__deepcopy__ = __deepcopy__\n",
    "            return result\n",
    "        # bind __deepcopy__ to the weightnorm'd layer\n",
    "        self.__deepcopy__ = __deepcopy__.__get__(self, self.__class__)\n",
    "\n",
    "\n",
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, n_modes, forecast_ff, backcast_ff,\n",
    "                 fourier_weight, factor, ff_weight_norm,\n",
    "                 n_ff_layers, layer_norm, use_fork, dropout, mode):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_modes = n_modes\n",
    "        self.mode = mode\n",
    "        self.use_fork = use_fork\n",
    "\n",
    "        self.fourier_weight = fourier_weight\n",
    "        # Can't use complex type yet. See https://github.com/pytorch/pytorch/issues/59998\n",
    "        if not self.fourier_weight:\n",
    "            self.fourier_weight = nn.ParameterList([])\n",
    "            for _ in range(2):\n",
    "                weight = torch.FloatTensor(in_dim, out_dim, n_modes, 2)\n",
    "                param = nn.Parameter(weight)\n",
    "                nn.init.xavier_normal_(param)\n",
    "                self.fourier_weight.append(param)\n",
    "\n",
    "        if use_fork:\n",
    "            self.forecast_ff = forecast_ff\n",
    "            if not self.forecast_ff:\n",
    "                self.forecast_ff = FeedForward(\n",
    "                    out_dim, factor, ff_weight_norm, n_ff_layers, layer_norm, dropout)\n",
    "\n",
    "        self.backcast_ff = backcast_ff\n",
    "        if not self.backcast_ff:\n",
    "            self.backcast_ff = FeedForward(\n",
    "                out_dim, factor, ff_weight_norm, n_ff_layers, layer_norm, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape == [batch_size, grid_size, grid_size, in_dim]\n",
    "        if self.mode != 'no-fourier':\n",
    "            x = self.forward_fourier(x)\n",
    "\n",
    "        b = self.backcast_ff(x)\n",
    "        f = self.forecast_ff(x) if self.use_fork else None\n",
    "        return b, f\n",
    "\n",
    "    def forward_fourier(self, x):\n",
    "        x = rearrange(x, 'b x i -> b i x')\n",
    "        # x.shape == [batch_size, in_dim, grid_size]\n",
    "\n",
    "        B, I, N = x.shape\n",
    "\n",
    "        # # # Dimesion X # # #\n",
    "        x_ft = torch.fft.rfft(x, dim=-1, norm='ortho')\n",
    "        # x_ft.shape == [batch_size, in_dim, grid_size, grid_size // 2 + 1]\n",
    "\n",
    "        out_ft = x_ft.new_zeros(B, I, N // 2 + 1)\n",
    "        # out_ft.shape == [batch_size, in_dim, grid_size, grid_size // 2 + 1, 2]\n",
    "        if self.mode == 'full':\n",
    "            out_ft[:, :, :self.n_modes] = torch.einsum(\n",
    "                \"bix,iox->box\",\n",
    "                x_ft[:, :, :self.n_modes],\n",
    "                torch.view_as_complex(self.fourier_weight[0]))\n",
    "        elif self.mode == 'low-pass':\n",
    "            out_ft[:, :, :self.n_modes] = x_ft[:, :, :self.n_modes]\n",
    "\n",
    "        x = torch.fft.irfft(out_ft, n=N, dim=-1, norm='ortho')\n",
    "        # x.shape == [batch_size, in_dim, grid_size, grid_size]\n",
    "\n",
    "\n",
    "\n",
    "        x = rearrange(x, 'b i x -> b x i')\n",
    "        # x.shape == [batch_size, grid_size, grid_size, out_dim]\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class F_FNO_1D(nn.Module):\n",
    "    def __init__(self, modes, width, input_dim=12, output_dim=1, dropout=0.0, in_dropout=0.0,\n",
    "                 n_layers=4, share_weight: bool = False,\n",
    "                 share_fork=False, factor=2,\n",
    "                 ff_weight_norm=False, n_ff_layers=2,\n",
    "                 gain=1, layer_norm=False, use_fork=False, mode='full'):\n",
    "        super().__init__()\n",
    "        self.modes = modes\n",
    "        self.width = width\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.in_proj = WNLinear(input_dim, self.width, wnorm=ff_weight_norm)\n",
    "        self.drop = nn.Dropout(in_dropout)\n",
    "        self.n_layers = n_layers\n",
    "        self.use_fork = use_fork\n",
    "        self.normalizer = Normalizer([self.input_dim], max_accumulations = 1000)\n",
    "\n",
    "        self.forecast_ff = self.backcast_ff = None\n",
    "        if share_fork:\n",
    "            if use_fork:\n",
    "                self.forecast_ff = FeedForward(\n",
    "                    width, factor, ff_weight_norm, n_ff_layers, layer_norm, dropout)\n",
    "            self.backcast_ff = FeedForward(\n",
    "                width, factor, ff_weight_norm, n_ff_layers, layer_norm, dropout)\n",
    "\n",
    "        self.fourier_weight = None\n",
    "        if share_weight:\n",
    "            self.fourier_weight = nn.ParameterList([])\n",
    "            for _ in range(2):\n",
    "                weight = torch.FloatTensor(width, width, modes, 2)\n",
    "                param = nn.Parameter(weight)\n",
    "                nn.init.xavier_normal_(param, gain=gain)\n",
    "                self.fourier_weight.append(param)\n",
    "\n",
    "        self.spectral_layers = nn.ModuleList([])\n",
    "        for _ in range(n_layers):\n",
    "            self.spectral_layers.append(SpectralConv1d(in_dim=width,\n",
    "                                                       out_dim=width,\n",
    "                                                       n_modes=modes,\n",
    "                                                       forecast_ff=self.forecast_ff,\n",
    "                                                       backcast_ff=self.backcast_ff,\n",
    "                                                       fourier_weight=self.fourier_weight,\n",
    "                                                       factor=factor,\n",
    "                                                       ff_weight_norm=ff_weight_norm,\n",
    "                                                       n_ff_layers=n_ff_layers,\n",
    "                                                       layer_norm=layer_norm,\n",
    "                                                       use_fork=use_fork,\n",
    "                                                       dropout=dropout,\n",
    "                                                       mode=mode))\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            WNLinear(self.width, 128, wnorm=ff_weight_norm),\n",
    "            WNLinear(128, self.output_dim, wnorm=ff_weight_norm))\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        # x.shape == [n_batches, *dim_sizes, input_size]\n",
    "        forecast = 0\n",
    "        # print(\"data_max -->\",x.max())\n",
    "        # print(\"data_min -->\",x.min())\n",
    "        x = self.normalizer(x)\n",
    "\n",
    "        print(\"mean\", x.mean())\n",
    "        print(\"std\", x.std())\n",
    "        #print(\"mean ->\", x.mean(0))\n",
    "\n",
    "\n",
    "        # print(\"data_max -->\",x.max())\n",
    "        # print(\"data_min -->\",x.min())\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"inverse\")\n",
    "        x_inv = self.normalizer.inverse(x, channel=0)\n",
    "\n",
    "        print(\"mean\", x_inv.mean())\n",
    "        print(\"std\", x_inv.std())\n",
    "\n",
    "        # print(\"data_max -->\",x_inv.max())\n",
    "        # print(\"data_min -->\",x_inv.min())\n",
    "\n",
    "        x = self.in_proj(x)\n",
    "        x = self.drop(x)\n",
    "        forecast_list = []\n",
    "        for i in range(self.n_layers):\n",
    "            layer = self.spectral_layers[i]\n",
    "            b, f = layer(x)\n",
    "\n",
    "            if self.use_fork:\n",
    "                f_out = self.out(f)\n",
    "                forecast = forecast + f_out\n",
    "                forecast_list.append(f_out)\n",
    "\n",
    "            x = x + b\n",
    "\n",
    "        if not self.use_fork:\n",
    "            forecast = self.out(b)\n",
    "\n",
    "        #forecast = self.normalizer.inverse(forecast, channel=0)\n",
    "        return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, modes, width, input_dim=12, output_dim=1, dropout=0.0, in_dropout=0.0,\n",
    "                 n_layers=4, share_weight: bool = False,\n",
    "                 share_fork=False, factor=2,\n",
    "                 ff_weight_norm=False, n_ff_layers=2,\n",
    "                 gain=1, layer_norm=False, use_fork=False, mode='full'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = F_FNO_1D(\n",
    "    modes = 16,\n",
    "    width = 64,\n",
    "    input_dim = 1,\n",
    "    output_dim = 1,\n",
    "    dropout=0.0,\n",
    "    \n",
    "    in_dropout=0.0,\n",
    "    n_layers=4,\n",
    "    share_weight = False,\n",
    "    \n",
    "    factor=2,\n",
    "    ff_weight_norm=True,\n",
    "    n_ff_layers=2,\n",
    "    gain=1,\n",
    "\n",
    "\n",
    "    layer_norm=False,\n",
    "    share_fork=False,\n",
    "    use_fork=False,\n",
    "    mode='full'\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1124418"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9554)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(5,100,1).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(5,100,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0141) tensor(0.9941)\n"
     ]
    }
   ],
   "source": [
    "print(data.mean(), data.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor(-0.6315)\n",
      "std tensor(0.3527)\n",
      "\n",
      "\n",
      "inverse\n",
      "mean tensor(0.0141)\n",
      "std tensor(0.9941)\n"
     ]
    }
   ],
   "source": [
    "output = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SpectralConv1d_Uno(nn.Module):\n",
    "    def __init__(self, in_codim, out_codim, dim1,modes1 = None):\n",
    "        super(SpectralConv1d_Uno, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        1D Fourier layer. It does FFT, linear transform, and Inverse FFT. \n",
    "        dim1 = Default output grid size along x (or 1st dimension of output domain) \n",
    "        Ratio of grid size of the input and the output implecitely \n",
    "        set the expansion or contraction farctor along each dimension of the domain.\n",
    "        modes1 = Number of fourier modes to consider for the integral operator.\n",
    "                Number of modes must be compatibale with the input grid size \n",
    "                and desired output grid size.\n",
    "                i.e., modes1 <= min( dim1/2, input_dim1/2). \n",
    "                Here \"input_dim1\" is the grid size along x axis (or first dimension) of the input domain.\n",
    "        in_codim = Input co-domian dimension\n",
    "        out_codim = output co-domain dimension\n",
    "        \"\"\"\n",
    "        in_codim = int(in_codim)\n",
    "        out_codim = int(out_codim)\n",
    "        self.in_channels = in_codim\n",
    "        self.out_channels = out_codim\n",
    "        self.dim1 = dim1 #output dimensions\n",
    "        if modes1 is not None:\n",
    "            self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        else:\n",
    "            self.modes1 = dim1//2\n",
    "\n",
    "        self.scale = (1 / (2*in_codim))**(1.0/2.0)\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.randn(in_codim, out_codim, self.modes1, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
    "        return torch.einsum(\"bix,iox->box\", input, weights)\n",
    "\n",
    "    def forward(self, x, dim1 = None):\n",
    "        \"\"\"\n",
    "        input shape = (batch, in_codim, input_dim1)\n",
    "        output shape = (batch, out_codim, dim1)\n",
    "        \"\"\"\n",
    "        #import pdb; pdb.set_trace()\n",
    "        if dim1 is not None:\n",
    "            self.dim1 = dim1\n",
    "        batchsize = x.shape[0]\n",
    "        \n",
    "        # print(\"conv...\")\n",
    "        # print(\"x_in ->\", x.shape)\n",
    "        x_ft = torch.fft.rfft(x, norm = 'forward')\n",
    "        #print(\"x_ft - >\", x_ft.shape)\n",
    "        # Multiply relevant Fourier modes\n",
    "        print(\"Out (xft, weight) ->\", x_ft[:, :, :self.modes1].shape,  self.weights1.shape )\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  self.dim1//2 + 1 , dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1] = self.compl_mul1d(x_ft[:, :, :self.modes1], self.weights1)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft(out_ft, n=self.dim1, norm = 'forward')\n",
    "        #print(\"x ->\", x.shape)\n",
    "        return x\n",
    "\n",
    "class pointwise_op_1D(nn.Module):\n",
    "    \"\"\"\n",
    "    All variables are consistent with the SpectralConv1d_Uno class.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_codim, out_codim,dim1):\n",
    "        super(pointwise_op_1D,self).__init__()\n",
    "        self.conv = nn.Conv1d(int(in_codim), int(out_codim), 1)\n",
    "        self.dim1 = int(dim1)\n",
    "\n",
    "    def forward(self,x, dim1 = None):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        if dim1 is None:\n",
    "            dim1 = self.dim1\n",
    "        x_out = self.conv(x)\n",
    "\n",
    "        #x_out = torch.nn.functional.interpolate(x_out, size = dim1,mode = 'linear',align_corners=True, antialias = True)\n",
    "        x_out = torch.nn.functional.interpolate(x_out, size = dim1,mode = 'linear',align_corners=True)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "\n",
    "class OperatorBlock_1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalize = if true performs InstanceNorm1d on the output.\n",
    "    Non_Lin = if true, applies point wise nonlinearity.\n",
    "    All other variables are consistent with the SpectralConv1d_Uno class.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_codim, out_codim,dim1,modes1, Normalize = True,Non_Lin = True):\n",
    "        super(OperatorBlock_1D,self).__init__()\n",
    "        self.conv = SpectralConv1d_Uno(in_codim, out_codim, dim1,modes1)\n",
    "        self.w = pointwise_op_1D(in_codim, out_codim, dim1)\n",
    "        self.normalize = Normalize\n",
    "        self.non_lin = Non_Lin\n",
    "        if Normalize:\n",
    "            self.normalize_layer = torch.nn.InstanceNorm1d(int(out_codim),affine=True)\n",
    "\n",
    "\n",
    "    def forward(self,x, dim1 = None):\n",
    "        \"\"\"\n",
    "        input shape = (batch, in_codim, input_dim1)\n",
    "        output shape = (batch, out_codim, dim1)\n",
    "        \"\"\"\n",
    "        # #import pdb; pdb.set_trace()\n",
    "        # print(\"\\n\")\n",
    "        # print(\"x_in ->\", x.shape)\n",
    "        x1_out = self.conv(x,dim1)\n",
    "\n",
    "        #print(\"x1_out ->\", x1_out.shape)\n",
    "\n",
    "        x2_out = self.w(x,dim1)\n",
    "\n",
    "        #print(\"x2_out ->\", x2_out.shape)\n",
    "        x_out = x1_out + x2_out\n",
    "        if self.normalize:\n",
    "            x_out = self.normalize_layer(x_out)\n",
    "        if self.non_lin:\n",
    "            x_out = F.gelu(x_out)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "\n",
    "# UNO model \n",
    "# it has less aggressive scaling factors for domains and co-domains.    \n",
    "class UNO_1D(nn.Module):\n",
    "    def __init__(self,in_width, width,pad = 0, factor = 3/4):\n",
    "        super(UNO_1D, self).__init__()\n",
    "\n",
    "\n",
    "        self.in_width = in_width # input channel\n",
    "        self.width = width \n",
    "        self.factor = factor\n",
    "        self.padding = pad  \n",
    "\n",
    "        self.fc = nn.Linear(self.in_width, self.width//2)\n",
    "\n",
    "        self.fc0 = nn.Linear(self.width//2, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "   \n",
    "        self.L0 = OperatorBlock_1D(self.width, 2*factor*self.width,64, 22)\n",
    "\n",
    "        self.L1 = OperatorBlock_1D(2*factor*self.width, 4*factor*self.width, 32, 14)\n",
    "\n",
    "        self.L2 = OperatorBlock_1D(4*factor*self.width, 8*factor*self.width, 16, 6,)\n",
    "        \n",
    "        self.L3 = OperatorBlock_1D(8*factor*self.width, 8*factor*self.width, 16, 6)\n",
    "        \n",
    "        self.L4 = OperatorBlock_1D(8*factor*self.width, 4*factor*self.width, 32, 6)\n",
    "\n",
    "        self.L5 = OperatorBlock_1D(8*factor*self.width, 2*factor*self.width, 48,14)\n",
    "\n",
    "        self.L6 = OperatorBlock_1D(4*factor*self.width, self.width, 64, 22) # will be reshaped\n",
    "\n",
    "        self.fc1 = nn.Linear(2*self.width, 4*self.width)\n",
    "        self.fc2 = nn.Linear(4*self.width, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        print(\"x ->\",x.shape)\n",
    "        print(\"grid ->\", grid.shape)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "\n",
    "        print(\"x + grid ->\",x.shape)\n",
    "\n",
    "\n",
    "        x_fc = self.fc(x)\n",
    "        x_fc = F.gelu(x_fc)\n",
    "\n",
    "        print(\"x_fc ->\",x_fc.shape)\n",
    "\n",
    "        x_fc0 = self.fc0(x_fc)\n",
    "        x_fc0 = F.gelu(x_fc0)\n",
    "        print(\"x_fc0 ->\",x_fc0.shape)\n",
    "\n",
    "\n",
    "        x_fc0 = x_fc0.permute(0, 2, 1)\n",
    "        \n",
    "        print(\"x_fc0 ->\",x_fc0.shape)\n",
    "        \n",
    "        x_fc0 = F.pad(x_fc0, [self.padding,self.padding, self.padding,self.padding])\n",
    "        \n",
    "        print(\"x_fc0 pad ->\",x_fc0.shape)\n",
    "\n",
    "        D1 = x_fc0.shape[-1]\n",
    "        \n",
    "        print(\"cofactor ->\", D1, self.factor, int(D1*self.factor))\n",
    "        x_c0 = self.L0(x_fc0,int(D1*self.factor))\n",
    "        print(\"x_c0 ->\",x_c0.shape)\n",
    "\n",
    "        x_c1 = self.L1(x_c0 ,D1//2)\n",
    "        print(\"x_c1 ->\",x_c1.shape)\n",
    "\n",
    "        x_c2 = self.L2(x_c1 ,D1//4)\n",
    "        print(\"x_c2 ->\",x_c2.shape)\n",
    "\n",
    "        x_c3 = self.L3(x_c2,D1//4)\n",
    "        print(\"x_c3 ->\",x_c3.shape)\n",
    "\n",
    "\n",
    "        x_c4 = self.L4(x_c3,D1//2)\n",
    "        print(\"x_c4 ->\",x_c4.shape)\n",
    "\n",
    "        x_c4 = torch.cat([x_c4, x_c1], dim=1)\n",
    "\n",
    "        x_c5 = self.L5(x_c4,int(D1*self.factor))\n",
    "        print(\"x_c5 ->\",x_c5.shape)\n",
    "        print(\"x_c0 ->\", x_c0.shape)\n",
    "        x_c5 = torch.cat([x_c5, x_c0], dim=1)\n",
    "\n",
    "        x_c6 = self.L6(x_c5,D1)\n",
    "        print(\"x_c6 ->\",x_c6.shape)\n",
    "        print(print(\"x_fc0 ->\", x_fc0.shape))\n",
    "        x_c6 = torch.cat([x_c6, x_fc0], dim=1)\n",
    "\n",
    "        if self.padding!=0:\n",
    "            x_c6 = x_c6[..., :-self.padding, :-self.padding]\n",
    "\n",
    "        x_c6 = x_c6.permute(0, 2, 1)\n",
    "        \n",
    "        x_fc1 = self.fc1(x_c6)\n",
    "        x_fc1 = F.gelu(x_fc1)\n",
    "        \n",
    "        x_out = self.fc2(x_fc1)\n",
    "        print(\"x_out ->\", x_out.shape)\n",
    "        return x_out\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 2*np.pi, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1).repeat([batchsize, 1, 1])\n",
    "        # gridy = torch.tensor(np.linspace(0, 2*np.pi, size_y), dtype=torch.float)\n",
    "        # gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        #return torch.cat((torch.sin(gridx),torch.sin(gridy),torch.cos(gridx),torch.cos(gridy)), dim=-1).to(device)\n",
    "        return (torch.sin(gridx) + torch.cos(gridx) ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(16,100,1)\n",
    "model = UNO_1D (\n",
    "    2,\n",
    "    32,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840593"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in (param for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=16, bias=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x -> torch.Size([16, 100, 1])\n",
      "grid -> torch.Size([16, 100, 1])\n",
      "x + grid -> torch.Size([16, 100, 2])\n",
      "x_fc -> torch.Size([16, 100, 16])\n",
      "x_fc0 -> torch.Size([16, 100, 32])\n",
      "x_fc0 -> torch.Size([16, 32, 100])\n",
      "x_fc0 pad -> torch.Size([16, 32, 100])\n",
      "cofactor -> 100 0.75 75\n",
      "Out (xft, weight) -> torch.Size([16, 32, 22]) torch.Size([32, 48, 22])\n",
      "x_c0 -> torch.Size([16, 48, 75])\n",
      "Out (xft, weight) -> torch.Size([16, 48, 14]) torch.Size([48, 96, 14])\n",
      "x_c1 -> torch.Size([16, 96, 50])\n",
      "Out (xft, weight) -> torch.Size([16, 96, 6]) torch.Size([96, 192, 6])\n",
      "x_c2 -> torch.Size([16, 192, 25])\n",
      "Out (xft, weight) -> torch.Size([16, 192, 6]) torch.Size([192, 192, 6])\n",
      "x_c3 -> torch.Size([16, 192, 25])\n",
      "Out (xft, weight) -> torch.Size([16, 192, 6]) torch.Size([192, 96, 6])\n",
      "x_c4 -> torch.Size([16, 96, 50])\n",
      "Out (xft, weight) -> torch.Size([16, 192, 14]) torch.Size([192, 48, 14])\n",
      "x_c5 -> torch.Size([16, 48, 75])\n",
      "x_c0 -> torch.Size([16, 48, 75])\n",
      "Out (xft, weight) -> torch.Size([16, 96, 22]) torch.Size([96, 32, 22])\n",
      "x_c6 -> torch.Size([16, 32, 100])\n",
      "x_fc0 -> torch.Size([16, 32, 100])\n",
      "None\n",
      "x_out -> torch.Size([16, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mordern UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT license.\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# from .activations import ACTIVATION_REGISTRY\n",
    "# from .fourier import SpectralConv2d\n",
    "\n",
    "# Largely based on https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/diffusion/ddpm/unet.py\n",
    "# MIT License\n",
    "# Copyright (c) 2020 Varuna Jayasiri\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Wide Residual Blocks used in modern Unet architectures.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        activation (str): Activation function to use.\n",
    "        norm (bool): Whether to use normalization.\n",
    "        n_groups (int): Number of groups for group normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        activation: str = \"gelu\",\n",
    "        norm: bool = False,\n",
    "        n_groups: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n",
    "        if self.activation is None:\n",
    "            raise NotImplementedError(f\"Activation {activation} not implemented\")\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "        # If the number of input channels is not equal to the number of output channels we have to\n",
    "        # project the shortcut connection\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        if norm:\n",
    "            self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
    "            self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
    "        else:\n",
    "            self.norm1 = nn.Identity()\n",
    "            self.norm2 = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # First convolution layer\n",
    "        h = self.conv1(self.activation(self.norm1(x)))\n",
    "        # Second convolution layer\n",
    "        h = self.conv2(self.activation(self.norm2(h)))\n",
    "        # Add the shortcut connection and return\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "\n",
    "class FourierResidualBlock(nn.Module):\n",
    "    \"\"\"Fourier Residual Block to be used in modern Unet architectures.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        modes1 (int): Number of modes in the first dimension.\n",
    "        modes2 (int): Number of modes in the second dimension.\n",
    "        activation (str): Activation function to use.\n",
    "        norm (bool): Whether to use normalization.\n",
    "        n_groups (int): Number of groups for group normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        modes1: int = 16,\n",
    "        modes2: int = 16,\n",
    "        activation: str = \"gelu\",\n",
    "        norm: bool = False,\n",
    "        n_groups: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n",
    "        if self.activation is None:\n",
    "            raise NotImplementedError(f\"Activation {activation} not implemented\")\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.fourier1 = SpectralConv2d(in_channels, out_channels, modes1=self.modes1, modes2=self.modes2)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, padding_mode=\"zeros\")\n",
    "        self.fourier2 = SpectralConv2d(out_channels, out_channels, modes1=self.modes1, modes2=self.modes2)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1, padding=0, padding_mode=\"zeros\")\n",
    "        # If the number of input channels is not equal to the number of output channels we have to\n",
    "        # project the shortcut connection\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        if norm:\n",
    "            self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
    "            self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
    "        else:\n",
    "            self.norm1 = nn.Identity()\n",
    "            self.norm2 = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # using pre-norms\n",
    "        h = self.activation(self.norm1(x))\n",
    "        x1 = self.fourier1(h)\n",
    "        x2 = self.conv1(h)\n",
    "        out = x1 + x2\n",
    "        out = self.activation(self.norm2(out))\n",
    "        x1 = self.fourier2(out)\n",
    "        x2 = self.conv2(out)\n",
    "        out = x1 + x2 + self.shortcut(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Attention block This is similar to [transformer multi-head\n",
    "    attention]\n",
    "\n",
    "    Args:\n",
    "        n_channels (int): the number of channels in the input\n",
    "        n_heads (int): the number of heads in multi-head attention\n",
    "        d_k: the number of dimensions in each head\n",
    "        n_groups (int): the number of groups for [group normalization][torch.nn.GroupNorm].\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, n_heads: int = 1, d_k: Optional[int] = None, n_groups: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Default `d_k`\n",
    "        if d_k is None:\n",
    "            d_k = n_channels\n",
    "        # Normalization layer\n",
    "        self.norm = nn.GroupNorm(n_groups, n_channels)\n",
    "        # Projections for query, key and values\n",
    "        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n",
    "        # Linear layer for final transformation\n",
    "        self.output = nn.Linear(n_heads * d_k, n_channels)\n",
    "        # Scale for dot-product attention\n",
    "        self.scale = d_k**-0.5\n",
    "        #\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Get shape\n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        # Change `x` to shape `[batch_size, seq, n_channels]`\n",
    "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
    "        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n",
    "        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
    "        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "        attn = torch.einsum(\"bihd,bjhd->bijh\", q, k) * self.scale\n",
    "        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
    "        attn = attn.softmax(dim=1)\n",
    "        # Multiply by values\n",
    "        res = torch.einsum(\"bijh,bjhd->bihd\", attn, v)\n",
    "        # Reshape to `[batch_size, seq, n_heads * d_k]`\n",
    "        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        # Transform to `[batch_size, seq, n_channels]`\n",
    "        res = self.output(res)\n",
    "\n",
    "        # Add skip connection\n",
    "        res += x\n",
    "\n",
    "        # Change to shape `[batch_size, in_channels, height, width]`\n",
    "        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
    "        return res\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"Down block This combines [`ResidualBlock`][pdearena.modules.twod_unet.ResidualBlock] and [`AttentionBlock`][pdearena.modules.twod_unet.AttentionBlock].\n",
    "\n",
    "    These are used in the first half of U-Net at each resolution.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels\n",
    "        out_channels (int): Number of output channels\n",
    "        has_attn (bool): Whether to use attention block\n",
    "        activation (nn.Module): Activation function\n",
    "        norm (bool): Whether to use normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        has_attn: bool = False,\n",
    "        activation: str = \"gelu\",\n",
    "        norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.res = ResidualBlock(in_channels, out_channels, activation=activation, norm=norm)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.res(x)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FourierDownBlock(nn.Module):\n",
    "    \"\"\"Down block This combines [`FourierResidualBlock`][pdearena.modules.twod_unet.FourierResidualBlock] and [`AttentionBlock`][pdearena.modules.twod_unet.AttentionBlock].\n",
    "\n",
    "    These are used in the first half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        modes1: int = 16,\n",
    "        modes2: int = 16,\n",
    "        has_attn: bool = False,\n",
    "        activation: str = \"gelu\",\n",
    "        norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.res = FourierResidualBlock(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            modes1=modes1,\n",
    "            modes2=modes2,\n",
    "            activation=activation,\n",
    "            norm=norm,\n",
    "        )\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.res(x)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"Up block that combines [`ResidualBlock`][pdearena.modules.twod_unet.ResidualBlock] and [`AttentionBlock`][pdearena.modules.twod_unet.AttentionBlock].\n",
    "\n",
    "    These are used in the second half of U-Net at each resolution.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels\n",
    "        out_channels (int): Number of output channels\n",
    "        has_attn (bool): Whether to use attention block\n",
    "        activation (str): Activation function\n",
    "        norm (bool): Whether to use normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        has_attn: bool = False,\n",
    "        activation: str = \"gelu\",\n",
    "        norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n",
    "        # from the first half of the U-Net\n",
    "        self.res = ResidualBlock(in_channels + out_channels, out_channels, activation=activation, norm=norm)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.res(x)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FourierUpBlock(nn.Module):\n",
    "    \"\"\"Up block that combines [`FourierResidualBlock`][pdearena.modules.twod_unet.FourierResidualBlock] and [`AttentionBlock`][pdearena.modules.twod_unet.AttentionBlock].\n",
    "\n",
    "    These are used in the second half of U-Net at each resolution.\n",
    "\n",
    "    Note:\n",
    "        We currently don't recommend using this block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        modes1: int = 16,\n",
    "        modes2: int = 16,\n",
    "        has_attn: bool = False,\n",
    "        activation: str = \"gelu\",\n",
    "        norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n",
    "        # from the first half of the U-Net\n",
    "        self.res = FourierResidualBlock(\n",
    "            in_channels + out_channels,\n",
    "            out_channels,\n",
    "            modes1=modes1,\n",
    "            modes2=modes2,\n",
    "            activation=activation,\n",
    "            norm=norm,\n",
    "        )\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.res(x)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiddleBlock(nn.Module):\n",
    "    \"\"\"Middle block\n",
    "\n",
    "    It combines a `ResidualBlock`, `AttentionBlock`, followed by another\n",
    "    `ResidualBlock`.\n",
    "\n",
    "    This block is applied at the lowest resolution of the U-Net.\n",
    "\n",
    "    Args:\n",
    "        n_channels (int): Number of channels in the input and output.\n",
    "        has_attn (bool, optional): Whether to use attention block. Defaults to False.\n",
    "        activation (str): Activation function to use. Defaults to \"gelu\".\n",
    "        norm (bool, optional): Whether to use normalization. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, has_attn: bool = False, activation: str = \"gelu\", norm: bool = False):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(n_channels, n_channels, activation=activation, norm=norm)\n",
    "        self.attn = AttentionBlock(n_channels) if has_attn else nn.Identity()\n",
    "        self.res2 = ResidualBlock(n_channels, n_channels, activation=activation, norm=norm)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.res1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.res2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    r\"\"\"Scale up the feature map by $2 \\times$\n",
    "\n",
    "    Args:\n",
    "        n_channels (int): Number of channels in the input and output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    r\"\"\"Scale down the feature map by $\\frac{1}{2} \\times$\n",
    "\n",
    "    Args:\n",
    "        n_channels (int): Number of channels in the input and output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    \"\"\"Modern U-Net architecture\n",
    "\n",
    "    This is a modern U-Net architecture with wide-residual blocks and spatial attention blocks\n",
    "\n",
    "    Args:\n",
    "        n_input_scalar_components (int): Number of scalar components in the model\n",
    "        n_input_vector_components (int): Number of vector components in the model\n",
    "        n_output_scalar_components (int): Number of output scalar components in the model\n",
    "        n_output_vector_components (int): Number of output vector components in the model\n",
    "        time_history (int): Number of time steps in the input\n",
    "        time_future (int): Number of time steps in the output\n",
    "        hidden_channels (int): Number of channels in the hidden layers\n",
    "        activation (str): Activation function to use\n",
    "        norm (bool): Whether to use normalization\n",
    "        ch_mults (list): List of channel multipliers for each resolution\n",
    "        is_attn (list): List of booleans indicating whether to use attention blocks\n",
    "        mid_attn (bool): Whether to use attention block in the middle block\n",
    "        n_blocks (int): Number of residual blocks in each resolution\n",
    "        use1x1 (bool): Whether to use 1x1 convolutions in the initial and final layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input_scalar_components: int,\n",
    "        n_input_vector_components: int,\n",
    "        n_output_scalar_components: int,\n",
    "        n_output_vector_components: int,\n",
    "        time_history: int,\n",
    "        time_future: int,\n",
    "        hidden_channels: int,\n",
    "        activation: str,\n",
    "        norm: bool = False,\n",
    "        ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "        is_attn: Union[Tuple[bool, ...], List[bool]] = (False, False, False, False),\n",
    "        mid_attn: bool = False,\n",
    "        n_blocks: int = 2,\n",
    "        use1x1: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_input_scalar_components = n_input_scalar_components\n",
    "        self.n_input_vector_components = n_input_vector_components\n",
    "        self.n_output_scalar_components = n_output_scalar_components\n",
    "        self.n_output_vector_components = n_output_vector_components\n",
    "        self.time_history = time_history\n",
    "        self.time_future = time_future\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n",
    "        if self.activation is None:\n",
    "            raise NotImplementedError(f\"Activation {activation} not implemented\")\n",
    "        # Number of resolutions\n",
    "        n_resolutions = len(ch_mults)\n",
    "\n",
    "        insize = time_history * (self.n_input_scalar_components + self.n_input_vector_components * 2)\n",
    "        n_channels = hidden_channels\n",
    "        # Project image into feature map\n",
    "        if use1x1:\n",
    "            self.image_proj = nn.Conv2d(insize, n_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.image_proj = nn.Conv2d(insize, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # #### First half of U-Net - decreasing resolution\n",
    "        down = []\n",
    "        # Number of channels\n",
    "        out_channels = in_channels = n_channels\n",
    "        # For each resolution\n",
    "        for i in range(n_resolutions):\n",
    "            # Number of output channels at this resolution\n",
    "            out_channels = in_channels * ch_mults[i]\n",
    "            # Add `n_blocks`\n",
    "            for _ in range(n_blocks):\n",
    "                down.append(\n",
    "                    DownBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        has_attn=is_attn[i],\n",
    "                        activation=activation,\n",
    "                        norm=norm,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "            # Down sample at all resolutions except the last\n",
    "            if i < n_resolutions - 1:\n",
    "                down.append(Downsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.down = nn.ModuleList(down)\n",
    "\n",
    "        # Middle block\n",
    "        self.middle = MiddleBlock(out_channels, has_attn=mid_attn, activation=activation, norm=norm)\n",
    "\n",
    "        # #### Second half of U-Net - increasing resolution\n",
    "        up = []\n",
    "        # Number of channels\n",
    "        in_channels = out_channels\n",
    "        # For each resolution\n",
    "        for i in reversed(range(n_resolutions)):\n",
    "            # `n_blocks` at the same resolution\n",
    "            out_channels = in_channels\n",
    "            for _ in range(n_blocks):\n",
    "                up.append(\n",
    "                    UpBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        has_attn=is_attn[i],\n",
    "                        activation=activation,\n",
    "                        norm=norm,\n",
    "                    )\n",
    "                )\n",
    "            # Final block to reduce the number of channels\n",
    "            out_channels = in_channels // ch_mults[i]\n",
    "            up.append(UpBlock(in_channels, out_channels, has_attn=is_attn[i], activation=activation, norm=norm))\n",
    "            in_channels = out_channels\n",
    "            # Up sample at all resolutions except last\n",
    "            if i > 0:\n",
    "                up.append(Upsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.up = nn.ModuleList(up)\n",
    "\n",
    "        if norm:\n",
    "            self.norm = nn.GroupNorm(8, n_channels)\n",
    "        else:\n",
    "            self.norm = nn.Identity()\n",
    "        out_channels = time_future * (self.n_output_scalar_components + self.n_output_vector_components * 2)\n",
    "        #\n",
    "        if use1x1:\n",
    "            self.final = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.final = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.dim() == 5\n",
    "        orig_shape = x.shape\n",
    "        x = x.reshape(x.size(0), -1, *x.shape[3:])  # collapse T,C\n",
    "        x = self.image_proj(x)\n",
    "\n",
    "        h = [x]\n",
    "        for m in self.down:\n",
    "            x = m(x)\n",
    "            h.append(x)\n",
    "\n",
    "        x = self.middle(x)\n",
    "\n",
    "        for m in self.up:\n",
    "            if isinstance(m, Upsample):\n",
    "                x = m(x)\n",
    "            else:\n",
    "                # Get the skip connection from first half of U-Net and concatenate\n",
    "                s = h.pop()\n",
    "                x = torch.cat((x, s), dim=1)\n",
    "                #\n",
    "                x = m(x)\n",
    "\n",
    "        x = self.final(self.activation(self.norm(x)))\n",
    "        x = x.reshape(\n",
    "            orig_shape[0], -1, (self.n_output_scalar_components + self.n_output_vector_components * 2), *orig_shape[3:]\n",
    "        )\n",
    "        return x\n",
    "\n",
    "\n",
    "class AltFourierUnet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input_scalar_components: int,\n",
    "        n_input_vector_components: int,\n",
    "        n_output_scalar_components: int,\n",
    "        n_output_vector_components: int,\n",
    "        time_history: int,\n",
    "        time_future: int,\n",
    "        hidden_channels: int,\n",
    "        activation: str,\n",
    "        modes1: int = 12,\n",
    "        modes2: int = 12,\n",
    "        norm: bool = False,\n",
    "        ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "        is_attn: Union[Tuple[bool, ...], List[bool]] = (False, False, False, False),\n",
    "        mid_attn: bool = False,\n",
    "        n_blocks: int = 2,\n",
    "        n_fourier_layers: int = 2,\n",
    "        mode_scaling: bool = True,\n",
    "        use1x1: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_input_scalar_components = n_input_scalar_components\n",
    "        self.n_input_vector_components = n_input_vector_components\n",
    "        self.n_output_scalar_components = n_output_scalar_components\n",
    "        self.n_output_vector_components = n_output_vector_components\n",
    "        self.time_history = time_history\n",
    "        self.time_future = time_future\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n",
    "        if self.activation is None:\n",
    "            raise NotImplementedError(f\"Activation {activation} not implemented\")\n",
    "\n",
    "        # Number of resolutions\n",
    "        n_resolutions = len(ch_mults)\n",
    "\n",
    "        insize = time_history * (self.n_input_scalar_components + self.n_input_vector_components * 2)\n",
    "        n_channels = hidden_channels\n",
    "        # Project image into feature map\n",
    "        if use1x1:\n",
    "            self.image_proj = nn.Conv2d(insize, n_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.image_proj = nn.Conv2d(insize, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # #### First half of U-Net - decreasing resolution\n",
    "        down = []\n",
    "        # Number of channels\n",
    "        out_channels = in_channels = n_channels\n",
    "        # For each resolution\n",
    "        for i in range(n_resolutions):\n",
    "            # Number of output channels at this resolution\n",
    "            out_channels = in_channels * ch_mults[i]\n",
    "            if i < n_fourier_layers:\n",
    "                for _ in range(n_blocks):\n",
    "                    down.append(\n",
    "                        FourierDownBlock(\n",
    "                            in_channels,\n",
    "                            out_channels,\n",
    "                            modes1=max(modes1 // 2**i, 4) if mode_scaling else modes1,\n",
    "                            modes2=max(modes2 // 2**i, 4) if mode_scaling else modes2,\n",
    "                            has_attn=is_attn[i],\n",
    "                            activation=activation,\n",
    "                            norm=norm,\n",
    "                        )\n",
    "                    )\n",
    "                    in_channels = out_channels\n",
    "            else:\n",
    "                # Add `n_blocks`\n",
    "                for _ in range(n_blocks):\n",
    "                    down.append(\n",
    "                        DownBlock(\n",
    "                            in_channels,\n",
    "                            out_channels,\n",
    "                            has_attn=is_attn[i],\n",
    "                            activation=activation,\n",
    "                            norm=norm,\n",
    "                        )\n",
    "                    )\n",
    "                    in_channels = out_channels\n",
    "            # Down sample at all resolutions except the last\n",
    "            if i < n_resolutions - 1:\n",
    "                down.append(Downsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.down = nn.ModuleList(down)\n",
    "\n",
    "        # Middle block\n",
    "        self.middle = MiddleBlock(out_channels, has_attn=mid_attn, activation=activation, norm=norm)\n",
    "\n",
    "        # #### Second half of U-Net - increasing resolution\n",
    "        up = []\n",
    "        # Number of channels\n",
    "        in_channels = out_channels\n",
    "        # For each resolution\n",
    "        for i in reversed(range(n_resolutions)):\n",
    "            # `n_blocks` at the same resolution\n",
    "            out_channels = in_channels\n",
    "            if i < n_fourier_layers:\n",
    "                for _ in range(n_blocks):\n",
    "                    up.append(\n",
    "                        FourierUpBlock(\n",
    "                            in_channels,\n",
    "                            out_channels,\n",
    "                            modes1=max(modes1 // 2**i, 4) if mode_scaling else modes1,\n",
    "                            modes2=max(modes2 // 2**i, 4) if mode_scaling else modes2,\n",
    "                            has_attn=is_attn[i],\n",
    "                            activation=activation,\n",
    "                            norm=norm,\n",
    "                        )\n",
    "                    )\n",
    "            else:\n",
    "                for _ in range(n_blocks):\n",
    "                    up.append(\n",
    "                        UpBlock(\n",
    "                            in_channels,\n",
    "                            out_channels,\n",
    "                            has_attn=is_attn[i],\n",
    "                            activation=activation,\n",
    "                            norm=norm,\n",
    "                        )\n",
    "                    )\n",
    "            # Final block to reduce the number of channels\n",
    "            out_channels = in_channels // ch_mults[i]\n",
    "            up.append(UpBlock(in_channels, out_channels, has_attn=is_attn[i], activation=activation, norm=norm))\n",
    "            in_channels = out_channels\n",
    "            # Up sample at all resolutions except last\n",
    "            if i > 0:\n",
    "                up.append(Upsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.up = nn.ModuleList(up)\n",
    "\n",
    "        if norm:\n",
    "            self.norm = nn.GroupNorm(8, n_channels)\n",
    "        else:\n",
    "            self.norm = nn.Identity()\n",
    "        out_channels = time_future * (self.n_output_scalar_components + self.n_output_vector_components * 2)\n",
    "        if use1x1:\n",
    "            self.final = nn.Conv2d(n_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.final = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.dim() == 5\n",
    "        orig_shape = x.shape\n",
    "        x = x.reshape(x.size(0), -1, *x.shape[3:])  # collapse T,C\n",
    "        x = self.image_proj(x)\n",
    "\n",
    "        h = [x]\n",
    "        for m in self.down:\n",
    "            x = m(x)\n",
    "            h.append(x)\n",
    "\n",
    "        x = self.middle(x)\n",
    "\n",
    "        for m in self.up:\n",
    "            if isinstance(m, Upsample):\n",
    "                x = m(x)\n",
    "            else:\n",
    "                # Get the skip connection from first half of U-Net and concatenate\n",
    "                s = h.pop()\n",
    "                x = torch.cat((x, s), dim=1)\n",
    "                #\n",
    "                x = m(x)\n",
    "\n",
    "        x = self.final(self.activation(self.norm(x)))\n",
    "        return x.reshape(\n",
    "            orig_shape[0], -1, (self.n_output_scalar_components + self.n_output_vector_components * 2), *orig_shape[3:]\n",
    "        )\n",
    "\n",
    "\n",
    "class FourierUnet(nn.Module):\n",
    "    \"\"\"Unet with Fourier layers in early downsampling blocks.\n",
    "\n",
    "    Args:\n",
    "        n_input_scalar_components (int): Number of scalar components in the model\n",
    "        n_input_vector_components (int): Number of vector components in the model\n",
    "        n_output_scalar_components (int): Number of output scalar components in the model\n",
    "        n_output_vector_components (int): Number of output vector components in the model\n",
    "        time_history (int): Number of time steps in the input.\n",
    "        time_future (int): Number of time steps in the output.\n",
    "        hidden_channels (int): Number of channels in the first layer.\n",
    "        activation (str): Activation function to use.\n",
    "        modes1 (int): Number of Fourier modes to use in the first spatial dimension.\n",
    "        modes2 (int): Number of Fourier modes to use in the second spatial dimension.\n",
    "        norm (bool): Whether to use normalization.\n",
    "        ch_mults (list): List of integers to multiply the number of channels by at each resolution.\n",
    "        is_attn (list): List of booleans indicating whether to use attention at each resolution.\n",
    "        mid_attn (bool): Whether to use attention in the middle block.\n",
    "        n_blocks (int): Number of blocks to use at each resolution.\n",
    "        n_fourier_layers (int): Number of early downsampling layers to use Fourier layers in.\n",
    "        mode_scaling (bool): Whether to scale the number of modes with resolution.\n",
    "        use1x1 (bool): Whether to use 1x1 convolutions in the initial and final layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input_scalar_components: int,\n",
    "        n_input_vector_components: int,\n",
    "        n_output_scalar_components: int,\n",
    "        n_output_vector_components: int,\n",
    "        time_history: int,\n",
    "        time_future: int,\n",
    "        hidden_channels: int,\n",
    "        activation: str,\n",
    "        modes1: int = 12,\n",
    "        modes2: int = 12,\n",
    "        norm: bool = False,\n",
    "        ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "        is_attn: Union[Tuple[bool, ...], List[bool]] = (False, False, False, False),\n",
    "        mid_attn: bool = False,\n",
    "        n_blocks: int = 2,\n",
    "        n_fourier_layers: int = 2,\n",
    "        mode_scaling: bool = True,\n",
    "        use1x1: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_input_scalar_components = n_input_scalar_components\n",
    "        self.n_input_vector_components = n_input_vector_components\n",
    "        self.n_output_scalar_components = n_output_scalar_components\n",
    "        self.n_output_vector_components = n_output_vector_components\n",
    "        self.time_history = time_history\n",
    "        self.time_future = time_future\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.activation: nn.Module = ACTIVATION_REGISTRY.get(activation, None)\n",
    "        if self.activation is None:\n",
    "            raise NotImplementedError(f\"Activation {activation} not implemented\")\n",
    "        # Number of resolutions\n",
    "        n_resolutions = len(ch_mults)\n",
    "\n",
    "        insize = time_history * (self.n_input_scalar_components + self.n_input_vector_components * 2)\n",
    "        n_channels = hidden_channels\n",
    "        # Project image into feature map\n",
    "        if use1x1:\n",
    "            self.image_proj = nn.Conv2d(insize, n_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.image_proj = nn.Conv2d(insize, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # #### First half of U-Net - decreasing resolution\n",
    "        down = []\n",
    "        # Number of channels\n",
    "        out_channels = in_channels = n_channels\n",
    "        # For each resolution\n",
    "        for i in range(n_resolutions):\n",
    "            # Number of output channels at this resolution\n",
    "            out_channels = in_channels * ch_mults[i]\n",
    "            if i < n_fourier_layers:\n",
    "                for _ in range(n_blocks):\n",
    "                    down.append(\n",
    "                        FourierDownBlock(\n",
    "                            in_channels,\n",
    "                            out_channels,\n",
    "                            modes1=max(modes1 // 2**i, 4) if mode_scaling else modes1,\n",
    "                            modes2=max(modes2 // 2**i, 4) if mode_scaling else modes2,\n",
    "                            has_attn=is_attn[i],\n",
    "                            activation=activation,\n",
    "                            norm=norm,\n",
    "                        )\n",
    "                    )\n",
    "                    in_channels = out_channels\n",
    "            else:\n",
    "                # Add `n_blocks`\n",
    "                for _ in range(n_blocks):\n",
    "                    down.append(\n",
    "                        DownBlock(\n",
    "                            in_channels,\n",
    "                            out_channels,\n",
    "                            has_attn=is_attn[i],\n",
    "                            activation=activation,\n",
    "                            norm=norm,\n",
    "                        )\n",
    "                    )\n",
    "                    in_channels = out_channels\n",
    "            # Down sample at all resolutions except the last\n",
    "            if i < n_resolutions - 1:\n",
    "                down.append(Downsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.down = nn.ModuleList(down)\n",
    "\n",
    "        # Middle block\n",
    "        self.middle = MiddleBlock(out_channels, has_attn=mid_attn, activation=activation, norm=norm)\n",
    "\n",
    "        # #### Second half of U-Net - increasing resolution\n",
    "        up = []\n",
    "        # Number of channels\n",
    "        in_channels = out_channels\n",
    "        # For each resolution\n",
    "        for i in reversed(range(n_resolutions)):\n",
    "            # `n_blocks` at the same resolution\n",
    "            out_channels = in_channels\n",
    "            for _ in range(n_blocks):\n",
    "                up.append(\n",
    "                    UpBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        has_attn=is_attn[i],\n",
    "                        activation=activation,\n",
    "                        norm=norm,\n",
    "                    )\n",
    "                )\n",
    "            # Final block to reduce the number of channels\n",
    "            out_channels = in_channels // ch_mults[i]\n",
    "            up.append(UpBlock(in_channels, out_channels, has_attn=is_attn[i], activation=activation, norm=norm))\n",
    "            in_channels = out_channels\n",
    "            # Up sample at all resolutions except last\n",
    "            if i > 0:\n",
    "                up.append(Upsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.up = nn.ModuleList(up)\n",
    "\n",
    "        if norm:\n",
    "            self.norm = nn.GroupNorm(8, n_channels)\n",
    "        else:\n",
    "            self.norm = nn.Identity()\n",
    "        out_channels = time_future * (self.n_output_scalar_components + self.n_output_vector_components * 2)\n",
    "        if use1x1:\n",
    "            self.final = nn.Conv2d(n_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.final = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.dim() == 5\n",
    "        orig_shape = x.shape\n",
    "        x = x.reshape(x.size(0), -1, *x.shape[3:])  # collapse T,C\n",
    "        x = self.image_proj(x)\n",
    "\n",
    "        h = [x]\n",
    "        for m in self.down:\n",
    "            x = m(x)\n",
    "            h.append(x)\n",
    "\n",
    "        x = self.middle(x)\n",
    "\n",
    "        for m in self.up:\n",
    "            if isinstance(m, Upsample):\n",
    "                x = m(x)\n",
    "            else:\n",
    "                # Get the skip connection from first half of U-Net and concatenate\n",
    "                s = h.pop()\n",
    "                x = torch.cat((x, s), dim=1)\n",
    "                #\n",
    "                x = m(x)\n",
    "\n",
    "        x = self.final(self.activation(self.norm(x)))\n",
    "        return x.reshape(\n",
    "            orig_shape[0], -1, (self.n_output_scalar_components + self.n_output_vector_components * 2), *orig_shape[3:]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morden UNET 1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT license.\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# from .activations import ACTIVATION_REGISTRY\n",
    "# from .fourier import SpectralConv2d\n",
    "\n",
    "# Largely based on https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/diffusion/ddpm/unet.py\n",
    "# MIT License\n",
    "# Copyright (c) 2020 Varuna Jayasiri\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Wide Residual Blocks used in modern Unet architectures.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        activation (str): Activation function to use.\n",
    "        norm (bool): Whether to use normalization.\n",
    "        n_groups (int): Number of groups for group normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        activation: str = \"gelu\",\n",
    "        norm: bool = False,\n",
    "        n_groups: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        if self.activation is None:\n",
    "            raise NotImplementedError(f\"Activation {activation} not implemented\")\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        # If the number of input channels is not equal to the number of output channels we have to\n",
    "        # project the shortcut connection\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        if norm:\n",
    "            self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
    "            self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
    "        else:\n",
    "            self.norm1 = nn.Identity()\n",
    "            self.norm2 = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # First convolution layer\n",
    "        h = self.conv1(self.activation(self.norm1(x)))\n",
    "        # Second convolution layer\n",
    "        h = self.conv2(self.activation(self.norm2(h)))\n",
    "        # Add the shortcut connection and return\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"Down block This combines [`ResidualBlock`][pdearena.modules.twod_unet.ResidualBlock] and [`AttentionBlock`][pdearena.modules.twod_unet.AttentionBlock].\n",
    "\n",
    "    These are used in the first half of U-Net at each resolution.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels\n",
    "        out_channels (int): Number of output channels\n",
    "        has_attn (bool): Whether to use attention block\n",
    "        activation (nn.Module): Activation function\n",
    "        norm (bool): Whether to use normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        has_attn: bool = False,\n",
    "        activation: str = \"gelu\",\n",
    "        norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.res = ResidualBlock(in_channels, out_channels, activation=activation, norm=norm)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.res(x)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"Up block that combines [`ResidualBlock`][pdearena.modules.twod_unet.ResidualBlock] and [`AttentionBlock`][pdearena.modules.twod_unet.AttentionBlock].\n",
    "\n",
    "    These are used in the second half of U-Net at each resolution.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels\n",
    "        out_channels (int): Number of output channels\n",
    "        has_attn (bool): Whether to use attention block\n",
    "        activation (str): Activation function\n",
    "        norm (bool): Whether to use normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        has_attn: bool = False,\n",
    "        activation: str = \"gelu\",\n",
    "        norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n",
    "        # from the first half of the U-Net\n",
    "        self.res = ResidualBlock(in_channels + out_channels, out_channels, activation=activation, norm=norm)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.res(x)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MiddleBlock(nn.Module):\n",
    "    \"\"\"Middle block\n",
    "\n",
    "    It combines a `ResidualBlock`, `AttentionBlock`, followed by another\n",
    "    `ResidualBlock`.\n",
    "\n",
    "    This block is applied at the lowest resolution of the U-Net.\n",
    "\n",
    "    Args:\n",
    "        n_channels (int): Number of channels in the input and output.\n",
    "        has_attn (bool, optional): Whether to use attention block. Defaults to False.\n",
    "        activation (str): Activation function to use. Defaults to \"gelu\".\n",
    "        norm (bool, optional): Whether to use normalization. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, has_attn: bool = False, activation: str = \"gelu\", norm: bool = False):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(n_channels, n_channels, activation=activation, norm=norm)\n",
    "        self.attn = AttentionBlock(n_channels) if has_attn else nn.Identity()\n",
    "        self.res2 = ResidualBlock(n_channels, n_channels, activation=activation, norm=norm)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.res1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.res2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    r\"\"\"Scale up the feature map by $2 \\times$\n",
    "\n",
    "    Args:\n",
    "        n_channels (int): Number of channels in the input and output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(n_channels, n_channels, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    r\"\"\"Scale down the feature map by $\\frac{1}{2} \\times$\n",
    "\n",
    "    Args:\n",
    "        n_channels (int): Number of channels in the input and output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(n_channels, n_channels, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class morden_Unet_1D(nn.Module):\n",
    "    \"\"\"Modern U-Net architecture\n",
    "\n",
    "    This is a modern U-Net architecture with wide-residual blocks and spatial attention blocks\n",
    "\n",
    "    Args:\n",
    "        n_input_scalar_components (int): Number of scalar components in the model\n",
    "        n_input_vector_components (int): Number of vector components in the model\n",
    "        n_output_scalar_components (int): Number of output scalar components in the model\n",
    "        n_output_vector_components (int): Number of output vector components in the model\n",
    "        time_history (int): Number of time steps in the input\n",
    "        time_future (int): Number of time steps in the output\n",
    "        hidden_channels (int): Number of channels in the hidden layers\n",
    "        activation (str): Activation function to use\n",
    "        norm (bool): Whether to use normalization\n",
    "        ch_mults (list): List of channel multipliers for each resolution\n",
    "        is_attn (list): List of booleans indicating whether to use attention blocks\n",
    "        mid_attn (bool): Whether to use attention block in the middle block\n",
    "        n_blocks (int): Number of residual blocks in each resolution\n",
    "        use1x1 (bool): Whether to use 1x1 convolutions in the initial and final layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input_scalar_components: int,\n",
    "        n_input_vector_components: int,\n",
    "        n_output_scalar_components: int,\n",
    "        n_output_vector_components: int,\n",
    "        time_history: int,\n",
    "        time_future: int,\n",
    "        hidden_channels: int,\n",
    "        activation: str,\n",
    "        norm: bool = False,\n",
    "        ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "        is_attn: Union[Tuple[bool, ...], List[bool]] = (False, False, False, False),\n",
    "        mid_attn: bool = False,\n",
    "        n_blocks: int = 2,\n",
    "        use1x1: bool = False,\n",
    "        padding = 14,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_input_scalar_components = n_input_scalar_components\n",
    "        self.n_input_vector_components = n_input_vector_components\n",
    "        self.n_output_scalar_components = n_output_scalar_components\n",
    "        self.n_output_vector_components = n_output_vector_components\n",
    "        self.time_history = time_history\n",
    "        self.time_future = time_future\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.padding = padding,\n",
    "\n",
    "        self.activation = activation\n",
    "        if self.activation is None:\n",
    "            raise NotImplementedError(f\"Activation {activation} not implemented\")\n",
    "        # Number of resolutions\n",
    "        n_resolutions = len(ch_mults)\n",
    "\n",
    "        insize = time_history * (self.n_input_scalar_components + self.n_input_vector_components * 2)\n",
    "        n_channels = hidden_channels\n",
    "        # Project image into feature map\n",
    "        if use1x1:\n",
    "            self.image_proj = nn.Conv1d(insize, n_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.image_proj = nn.Conv1d(insize, n_channels, kernel_size=1, padding=1)\n",
    "\n",
    "        # #### First half of U-Net - decreasing resolution\n",
    "        down = []\n",
    "        # Number of channels\n",
    "        out_channels = in_channels = n_channels\n",
    "        # For each resolution\n",
    "        for i in range(n_resolutions):\n",
    "            # Number of output channels at this resolution\n",
    "            out_channels = in_channels * ch_mults[i]\n",
    "            # Add `n_blocks`\n",
    "            for _ in range(n_blocks):\n",
    "                down.append(\n",
    "                    DownBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        has_attn=is_attn[i],\n",
    "                        activation=activation,\n",
    "                        norm=norm,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "            # Down sample at all resolutions except the last\n",
    "            if i < n_resolutions - 1:\n",
    "                down.append(Downsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.down = nn.ModuleList(down)\n",
    "\n",
    "        # Middle block\n",
    "        self.middle = MiddleBlock(out_channels, has_attn=mid_attn, activation=activation, norm=norm)\n",
    "\n",
    "        # #### Second half of U-Net - increasing resolution\n",
    "        up = []\n",
    "        # Number of channels\n",
    "        in_channels = out_channels\n",
    "        # For each resolution\n",
    "        for i in reversed(range(n_resolutions)):\n",
    "            # `n_blocks` at the same resolution\n",
    "            out_channels = in_channels\n",
    "            for _ in range(n_blocks):\n",
    "                up.append(\n",
    "                    UpBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        has_attn=is_attn[i],\n",
    "                        activation=activation,\n",
    "                        norm=norm,\n",
    "                    )\n",
    "                )\n",
    "            # Final block to reduce the number of channels\n",
    "            out_channels = in_channels // ch_mults[i]\n",
    "            up.append(UpBlock(in_channels, out_channels, has_attn=is_attn[i], activation=activation, norm=norm))\n",
    "            in_channels = out_channels\n",
    "            # Up sample at all resolutions except last\n",
    "            if i > 0:\n",
    "                up.append(Upsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.up = nn.ModuleList(up)\n",
    "\n",
    "        if norm:\n",
    "            self.norm = nn.GroupNorm(8, n_channels)\n",
    "        else:\n",
    "            self.norm = nn.Identity()\n",
    "        out_channels = time_future * (self.n_output_scalar_components + self.n_output_vector_components * 2)\n",
    "        #\n",
    "        if use1x1:\n",
    "            self.final = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.final = nn.Conv1d(in_channels, out_channels, kernel_size=1, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # assert x.dim() == 5\n",
    "        # orig_shape = x.shape\n",
    "        # x = x.reshape(x.size(0), -1, *x.shape[3:])  # collapse T,C\n",
    "        #print(x.shape)\n",
    "        #print(self.padding)\n",
    "\n",
    "        orig_shape = x.shape\n",
    "        \n",
    "        x = F.pad(x.permute(0,2,1), [self.padding[0], self.padding[0] ]).permute(0,2,1)\n",
    "\n",
    "        #print(\"x_pad ->\", x.shape)\n",
    "        \n",
    "\n",
    "        \n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "\n",
    "        x = self.image_proj(x)\n",
    "        #print(\"x -->\", x.shape)\n",
    "        h = [x]\n",
    "        for m in self.down:\n",
    "            x = m(x)\n",
    "            h.append(x)\n",
    "\n",
    "        x = self.middle(x)\n",
    "\n",
    "        for m in self.up:\n",
    "            if isinstance(m, Upsample):\n",
    "                x = m(x)\n",
    "            else:\n",
    "                # Get the skip connection from first half of U-Net and concatenate\n",
    "                s = h.pop()\n",
    "                x = torch.cat((x, s), dim=1)\n",
    "                x = m(x)\n",
    "\n",
    "        x = self.final(self.activation(self.norm(x)))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        x = x[...,self.padding[0]: -self.padding[0]]\n",
    "\n",
    "        x = x.reshape(\n",
    "            orig_shape[0], -1, (self.n_output_scalar_components + self.n_output_vector_components * 2), *orig_shape[3:]\n",
    "        )\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = morden_Unet_1D(\n",
    "        n_input_scalar_components = 1,\n",
    "        n_input_vector_components = 0,\n",
    "        n_output_scalar_components = 1,\n",
    "        n_output_vector_components = 0,\n",
    "        time_history = 1,\n",
    "        time_future = 1,\n",
    "        hidden_channels = 16,\n",
    "        activation = nn.GELU(),\n",
    "        norm = True,\n",
    "        ch_mults = (1, 2, 3, 4), #: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "        is_attn = (False, False, False, False),  #Union[Tuple[bool, ...], List[bool]] = (False, False, False, False),\n",
    "        mid_attn = False, #: bool = False,\n",
    "        n_blocks = 1, #: int = 2,\n",
    "        use1x1 = True, #: bool = False,\n",
    "        padding = 14,\n",
    "\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = torch.rand(5,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4469761"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in (param for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
